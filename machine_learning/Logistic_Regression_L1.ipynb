{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d985cd35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.875\n",
      "0.75\n"
     ]
    }
   ],
   "source": [
    "# ## Question 1: The Reality Check (Logistic Regression) üõ°Ô∏è\n",
    "# Let's revisit our Customer Churn scenario, but do it properly this time. We will split the data: 80% for training and 20% for testing.\n",
    "\n",
    "# Dataset:\n",
    "\n",
    "# Python\n",
    "\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# data1 = {\n",
    "#     'Account_Age_Months': [12, 2, 45, 1, 6, 34, 20, 2, 10, 60, 4, 8, 25, 3, 50, 5, 15, 1, 30, 40],\n",
    "#     'Monthly_Bill': [50, 80, 60, 90, 40, 100, 70, 85, 55, 110, 95, 45, 65, 88, 105, 50, 75, 92, 60, 80],\n",
    "#     'Churn': [0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0] # 1 = Left the company\n",
    "# }\n",
    "# df1 = pd.DataFrame(data1)\n",
    "# Your Tasks:\n",
    "\n",
    "# Split the Data: Define X and y. Then, use train_test_split (import it first!) to create four new arrays: X_train, X_test, y_train, y_test. Set test_size=0.2 and random_state=42 (this ensures we get the same split every time).\n",
    "\n",
    "# Train: Fit your LogisticRegression model ONLY on X_train and y_train.\n",
    "\n",
    "# Predict:\n",
    "\n",
    "# Predict on the training set (X_train) and calculate Training Accuracy.\n",
    "\n",
    "# Predict on the testing set (X_test) and calculate Testing Accuracy.\n",
    "\n",
    "# Compare: Look at the two scores. Is the Training Accuracy higher, lower, or the same as Testing Accuracy? What might it mean if Training Accuracy is 100% but Testing Accuracy is 50%?\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data1 = {\n",
    "    'Account_Age_Months': [12, 2, 45, 1, 6, 34, 20, 2, 10, 60, 4, 8, 25, 3, 50, 5, 15, 1, 30, 40],\n",
    "    'Monthly_Bill': [50, 80, 60, 90, 40, 100, 70, 85, 55, 110, 95, 45, 65, 88, 105, 50, 75, 92, 60, 80],\n",
    "    'Churn': [0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0] # 1 = Left the company\n",
    "}\n",
    "df = pd.DataFrame(data1)\n",
    "# display(df)\n",
    "\n",
    "x = df[['Account_Age_Months', 'Monthly_Bill']]\n",
    "y = df['Churn']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "model.fit(x_train,y_train)\n",
    "\n",
    "y_train_pred = model.predict(x_train)\n",
    "y_test_pred = model.predict(x_test)\n",
    "\n",
    "\n",
    "print(accuracy_score(y_train,y_train_pred))\n",
    "print(accuracy_score(y_test,y_test_pred))\n",
    "\n",
    "#the train data accuracy comes out to be 87.5 percent whereas the test data accuracy comes out to be 75% which is lower. meaning the model made predictions 87.5 percent of the times correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1abc849e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# ## Question 2: Visualizing Overfitting with Decision Trees üå≥\n",
    "# To understand why XGBoost is necessary, we first need to look at Decision Trees. Trees are powerful but prone to Overfitting‚Äîthey can create infinite complex rules to memorize the training data.\n",
    "\n",
    "# Dataset:\n",
    "\n",
    "# Python\n",
    "\n",
    "# import pandas as pd\n",
    "# data2 = {\n",
    "#     'Credit_Score': [600, 800, 650, 700, 720, 550, 580, 780, 820, 690, 610, 750, 640, 710, 590],\n",
    "#     'Income_k': [30, 80, 40, 60, 65, 25, 35, 75, 90, 55, 32, 70, 45, 58, 28],\n",
    "#     'Approved': [0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0]\n",
    "# }\n",
    "# df2 = pd.DataFrame(data2)\n",
    "# Your Tasks:\n",
    "\n",
    "# Split: Perform an 80/20 train/test split (random_state=42).\n",
    "\n",
    "# The \"Overfit\" Model: Import DecisionTreeClassifier from sklearn.tree. Create a model with no limits (model = DecisionTreeClassifier()) and train it on the training data.\n",
    "\n",
    "# Evaluate: Calculate the Training Accuracy and Testing Accuracy.\n",
    "\n",
    "# The \"Generalized\" Model: Create a second tree, but this time limit its depth: model_constrained = DecisionTreeClassifier(max_depth=2). Train and evaluate this one.\n",
    "\n",
    "# Analyze: Compare the gap between Train/Test scores for both models. Which model would you trust more in production, and why?\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "data2 = {\n",
    "    'Credit_Score': [600, 800, 650, 700, 720, 550, 580, 780, 820, 690, 610, 750, 640, 710, 590],\n",
    "    'Income_k': [30, 80, 40, 60, 65, 25, 35, 75, 90, 55, 32, 70, 45, 58, 28],\n",
    "    'Approved': [0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0]\n",
    "}\n",
    "df = pd.DataFrame(data2)\n",
    "\n",
    "x = df[['Credit_Score','Income_k']]\n",
    "y = df['Approved']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(x_train,y_train)\n",
    "\n",
    "y_train_pred = model.predict(x_train)\n",
    "y_test_pred = model.predict(x_test)\n",
    "\n",
    "print(accuracy_score(y_train,y_train_pred))\n",
    "print(accuracy_score(y_test,y_test_pred))\n",
    "#the accuracy score is 100% for both testing and training data\n",
    "\n",
    "model_2_constrained = DecisionTreeClassifier(max_depth=2)\n",
    "model_2_constrained.fit(x_train,y_train)\n",
    "\n",
    "y_train_pred_constrained = model_2_constrained.predict(x_train)\n",
    "y_test_pred_constrained = model_2_constrained.predict(x_test)\n",
    "\n",
    "print(accuracy_score(y_train,y_train_pred_constrained))\n",
    "print(accuracy_score(y_test,y_test_pred_constrained))\n",
    "\n",
    "#less data, code's fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277cf686",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
