{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d985cd35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.875\n",
      "0.75\n"
     ]
    }
   ],
   "source": [
    "# ## Question 1: The Reality Check (Logistic Regression) üõ°Ô∏è\n",
    "# Let's revisit our Customer Churn scenario, but do it properly this time. We will split the data: 80% for training and 20% for testing.\n",
    "\n",
    "# Dataset:\n",
    "\n",
    "# Python\n",
    "\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# data1 = {\n",
    "#     'Account_Age_Months': [12, 2, 45, 1, 6, 34, 20, 2, 10, 60, 4, 8, 25, 3, 50, 5, 15, 1, 30, 40],\n",
    "#     'Monthly_Bill': [50, 80, 60, 90, 40, 100, 70, 85, 55, 110, 95, 45, 65, 88, 105, 50, 75, 92, 60, 80],\n",
    "#     'Churn': [0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0] # 1 = Left the company\n",
    "# }\n",
    "# df1 = pd.DataFrame(data1)\n",
    "# Your Tasks:\n",
    "\n",
    "# Split the Data: Define X and y. Then, use train_test_split (import it first!) to create four new arrays: X_train, X_test, y_train, y_test. Set test_size=0.2 and random_state=42 (this ensures we get the same split every time).\n",
    "\n",
    "# Train: Fit your LogisticRegression model ONLY on X_train and y_train.\n",
    "\n",
    "# Predict:\n",
    "\n",
    "# Predict on the training set (X_train) and calculate Training Accuracy.\n",
    "\n",
    "# Predict on the testing set (X_test) and calculate Testing Accuracy.\n",
    "\n",
    "# Compare: Look at the two scores. Is the Training Accuracy higher, lower, or the same as Testing Accuracy? What might it mean if Training Accuracy is 100% but Testing Accuracy is 50%?\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data1 = {\n",
    "    'Account_Age_Months': [12, 2, 45, 1, 6, 34, 20, 2, 10, 60, 4, 8, 25, 3, 50, 5, 15, 1, 30, 40],\n",
    "    'Monthly_Bill': [50, 80, 60, 90, 40, 100, 70, 85, 55, 110, 95, 45, 65, 88, 105, 50, 75, 92, 60, 80],\n",
    "    'Churn': [0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0] # 1 = Left the company\n",
    "}\n",
    "df = pd.DataFrame(data1)\n",
    "# display(df)\n",
    "\n",
    "x = df[['Account_Age_Months', 'Monthly_Bill']]\n",
    "y = df['Churn']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "model.fit(x_train,y_train)\n",
    "\n",
    "y_train_pred = model.predict(x_train)\n",
    "y_test_pred = model.predict(x_test)\n",
    "\n",
    "\n",
    "print(accuracy_score(y_train,y_train_pred))\n",
    "print(accuracy_score(y_test,y_test_pred))\n",
    "\n",
    "#the train data accuracy comes out to be 87.5 percent whereas the test data accuracy comes out to be 75% which is lower. meaning the model made predictions 87.5 percent of the times correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1abc849e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# ## Question 2: Visualizing Overfitting with Decision Trees üå≥\n",
    "# To understand why XGBoost is necessary, we first need to look at Decision Trees. Trees are powerful but prone to Overfitting‚Äîthey can create infinite complex rules to memorize the training data.\n",
    "\n",
    "# Dataset:\n",
    "\n",
    "# Python\n",
    "\n",
    "# import pandas as pd\n",
    "# data2 = {\n",
    "#     'Credit_Score': [600, 800, 650, 700, 720, 550, 580, 780, 820, 690, 610, 750, 640, 710, 590],\n",
    "#     'Income_k': [30, 80, 40, 60, 65, 25, 35, 75, 90, 55, 32, 70, 45, 58, 28],\n",
    "#     'Approved': [0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0]\n",
    "# }\n",
    "# df2 = pd.DataFrame(data2)\n",
    "# Your Tasks:\n",
    "\n",
    "# Split: Perform an 80/20 train/test split (random_state=42).\n",
    "\n",
    "# The \"Overfit\" Model: Import DecisionTreeClassifier from sklearn.tree. Create a model with no limits (model = DecisionTreeClassifier()) and train it on the training data.\n",
    "\n",
    "# Evaluate: Calculate the Training Accuracy and Testing Accuracy.\n",
    "\n",
    "# The \"Generalized\" Model: Create a second tree, but this time limit its depth: model_constrained = DecisionTreeClassifier(max_depth=2). Train and evaluate this one.\n",
    "\n",
    "# Analyze: Compare the gap between Train/Test scores for both models. Which model would you trust more in production, and why?\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "data2 = {\n",
    "    'Credit_Score': [600, 800, 650, 700, 720, 550, 580, 780, 820, 690, 610, 750, 640, 710, 590],\n",
    "    'Income_k': [30, 80, 40, 60, 65, 25, 35, 75, 90, 55, 32, 70, 45, 58, 28],\n",
    "    'Approved': [0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0]\n",
    "}\n",
    "df = pd.DataFrame(data2)\n",
    "\n",
    "x = df[['Credit_Score','Income_k']]\n",
    "y = df['Approved']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(x_train,y_train)\n",
    "\n",
    "y_train_pred = model.predict(x_train)\n",
    "y_test_pred = model.predict(x_test)\n",
    "\n",
    "print(accuracy_score(y_train,y_train_pred))\n",
    "print(accuracy_score(y_test,y_test_pred))\n",
    "#the accuracy score is 100% for both testing and training data\n",
    "\n",
    "model_2_constrained = DecisionTreeClassifier(max_depth=2)\n",
    "model_2_constrained.fit(x_train,y_train)\n",
    "\n",
    "y_train_pred_constrained = model_2_constrained.predict(x_train)\n",
    "y_test_pred_constrained = model_2_constrained.predict(x_test)\n",
    "\n",
    "print(accuracy_score(y_train,y_train_pred_constrained))\n",
    "print(accuracy_score(y_test,y_test_pred_constrained))\n",
    "\n",
    "#less data, code's fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "277cf686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.5\n",
      "[0.26514524 0.28199276 0.2095659  0.24329616]\n"
     ]
    }
   ],
   "source": [
    "# ## Question 3: Enter the Beast - XGBoost üöÄ\n",
    "# Now we use the industry standard. XGBoost (Extreme Gradient Boosting) is an ensemble method. Instead of building one tree, it builds hundreds of small trees, where each new tree tries to correct the errors of the previous one.\n",
    "\n",
    "# Note: You may need to run pip install xgboost in your terminal first.\n",
    "\n",
    "# Dataset: We will use a slightly larger dataset for this, as XGBoost loves data.\n",
    "\n",
    "# Python\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# # Generating a synthetic dataset\n",
    "# np.random.seed(42)\n",
    "# data3 = {\n",
    "#     'Age': np.random.randint(18, 70, 100),\n",
    "#     'Salary_k': np.random.randint(30, 150, 100),\n",
    "#     'Savings_k': np.random.randint(0, 50, 100),\n",
    "#     'Debt_k': np.random.randint(0, 30, 100),\n",
    "#     # Target: 1 if bought product, 0 if not\n",
    "#     'Bought_Product': np.random.randint(0, 2, 100) \n",
    "# }\n",
    "# df3 = pd.DataFrame(data3)\n",
    "# Your Tasks:\n",
    "\n",
    "# Split: Perform an 80/20 train/test split (random_state=42).\n",
    "\n",
    "# Train XGBoost:\n",
    "\n",
    "# Import the classifier: from xgboost import XGBClassifier\n",
    "\n",
    "# Create the model instance. (Standard tip: XGBoost can be chatty, so you can use model = XGBClassifier(eval_metric='logloss') to keep it quiet).\n",
    "\n",
    "# Fit it on X_train and y_train.\n",
    "\n",
    "# Evaluate: Calculate the Training Accuracy and Testing Accuracy.\n",
    "\n",
    "# Comparison: How does the Test Accuracy compare to the Train Accuracy? Is the gap large?\n",
    "\n",
    "# Industry check: XGBoost outputs \"Feature Importances\" which tell you which columns drove the decision. Run print(model.feature_importances_). Which feature (index 0, 1, 2, or 3) was the most important for predicting if someone bought the product?\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "data3 = {\n",
    "    'Age': np.random.randint(18, 70, 100),\n",
    "    'Salary_k': np.random.randint(30, 150, 100),\n",
    "    'Savings_k': np.random.randint(0, 50, 100),\n",
    "    'Debt_k': np.random.randint(0, 30, 100),\n",
    "    # Target: 1 if bought product, 0 if not\n",
    "    'Bought_Product': np.random.randint(0, 2, 100) \n",
    "}\n",
    "df = pd.DataFrame(data3)\n",
    "\n",
    "x = df[['Age', 'Salary_k', 'Savings_k', 'Debt_k']]\n",
    "y = df['Bought_Product']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "model = XGBClassifier()\n",
    "model.fit(x_train,y_train)\n",
    "\n",
    "y_train_pred = model.predict(x_train)\n",
    "y_test_pred = model.predict(x_test)\n",
    "\n",
    "print(accuracy_score(y_train, y_train_pred))\n",
    "print(accuracy_score(y_test, y_test_pred))\n",
    "#both showing up 100% accuracy feels like an overfit, cant decide\n",
    "\n",
    "print(model.feature_importances_) #looking at the coefficients i can say that Salary_k has the most contribution on deciding if the customer bought or not"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
